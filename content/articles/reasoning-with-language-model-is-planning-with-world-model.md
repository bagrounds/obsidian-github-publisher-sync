---
share: true
aliases:
  - ğŸ—£ï¸ğŸ—ºï¸ğŸ¤–âš™ï¸ Reasoning with Language Model is Planning with World Model
title: ğŸ—£ï¸ğŸ—ºï¸ğŸ¤–âš™ï¸ Reasoning with Language Model is Planning with World Model
URL: https://bagrounds.org/articles/reasoning-with-language-model-is-planning-with-world-model
Author: 
tags: []
---
[Home](../index.md) > [Articles](./index.md)  
# [ğŸ—£ï¸ğŸ—ºï¸ğŸ¤–âš™ï¸ Reasoning with Language Model is Planning with World Model](https://arxiv.org/pdf/2305.14992)  
## ğŸ¤– AI Summary  
This paper outlines a new framework, Reasoning via Planning (RAP). It argues that [ğŸ¤–ğŸ¦œ Large Language Models (LLMs)](../topics/large-language-models.md) sometimes ğŸ¤¯ struggle with problems that are easy for humans, such as generating action plans, complex math, or logical reasoning.  
  
* This ğŸ˜¢ deficiency stems from the fact that LLMs lack an internal world model to predict the world state and simulate long-term outcomes.  
* The ğŸ’¡ solution proposed is a new LLM reasoning framework called Reasoning via Planning (RAP).  
* RAP repurposes the ğŸ¤– LLM as both a world model and a reasoning agent.  
* The framework incorporates a principled planning algorithm based on Monte Carlo Tree Search for ğŸ—ºï¸ strategic exploration of the reasoning space.  
* The paper ğŸ“ˆ demonstrates RAP's superiority over strong baselines on challenging reasoning problems, including plan generation, math reasoning, and logical inference.  
* In one plan generation setting, RAP with LLaMA-33B even ğŸ‘‘ surpasses CoT with GPT-4, achieving a 33% relative improvement.  
  
## ğŸ¤” Evaluation  
The paper ğŸ§ contrasts the new framework with existing methods, primarily Chain-of-Thought (CoT), arguing that current LLM reasoning is "instinctively" autoregressive, which is in stark contrast to the deliberate planning enabled by RAP. RAP's approach formally introduces a world model, reward mechanisms, and state into a unified framework, which the authors claim other search-guided methods lack. For further understanding, the paper suggests a few areas to explore. It would be interesting to see how ğŸ› ï¸ fine-tuning LLMs could enhance their reasoning and world model capabilities. Additionally, combining RAP with ğŸ¤ external tools is an identified path for solving more complex real-world problems. The paper also highlights that the combination of multiple rewards improves performance, but the specific effects depend on the nature of the task.  
  
## ğŸ“š Book Recommendations  
* [ğŸ¤–ğŸ§  Artificial Intelligence: A Modern Approach](../books/artificial-intelligence-a-modern-approach.md) by Stuart Russell and Peter Norvig: A foundational ğŸ“ text on planning algorithms and intelligent agents, relevant to the RAP framework.  
* [ğŸ¤”ğŸ‡ğŸ¢ Thinking, Fast and Slow](../books/thinking-fast-and-slow.md) by Daniel Kahneman: Explores human thought systemsâ€”intuitive (fast) and deliberate (slow)â€”offering a ğŸ§  contrast to the paper's comparison of LLM reasoning.  
* [ğŸ¤–â•ğŸ§ â¡ï¸ Reinforcement Learning: An Introduction](../books/reinforcement-learning-an-introduction.md) by Richard S. Sutton and Andrew G. Barto: A classic text on reinforcement learning, providing the theoretical underpinnings for the reward-based planning and ğŸ¯ strategic exploration used in RAP.  
* The Alignment Problem by Brian Christian: Addresses the critical question of how to ensure machine learning systems âš–ï¸ align with human values.  
* Build a Large Language Model (From Scratch) by Sebastian Raschka: A hands-on guide for those who want to ğŸ› ï¸ build a large language model from the ground up.  
* AI Superpowers: China, Silicon Valley, and the New World Order by Kai-Fu Lee: Offers a broader geopolitical ğŸŒ perspective on the global competition in artificial intelligence.  
* Multi-Agent Reinforcement Learning: Foundations and Modern Approaches by Stefano V. Albrecht, Filippos Christianos, and Lukas SchÃ¤fer: Dives into how multiple intelligent agents can ğŸ¤ interact and learn in shared environments.  
  
## ğŸ¦ Tweet  
<blockquote class="twitter-tweet" data-theme="dark"><p lang="en" dir="ltr">ğŸ—£ï¸ğŸ—ºï¸ğŸ¤–âš™ï¸ Reasoning with Language Model is Planning with World Model<br><br>ğŸ§  Reasoning | ğŸ—ºï¸ Planning | ğŸ¤– Language Models | ğŸ“ˆ Performance | ğŸ’¡ Framework | ğŸ¤– World Model | ğŸ—„ï¸ Arxiv<a href="https://t.co/o2QQgTuDxO">https://t.co/o2QQgTuDxO</a></p>&mdash; Bryan Grounds (@bagrounds) <a href="https://twitter.com/bagrounds/status/1951088746855801047?ref_src=twsrc%5Etfw">August 1, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>