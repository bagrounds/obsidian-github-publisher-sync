---
share: true
aliases:
  - 2025-01-12
title: 2025-01-12
URL: https://bagrounds.org/reflections/2025-01-12
Author: "[[bryan-grounds]]"
tags: 
---
[Home](../index.md) > [Reflections](./index.md) | [‚èÆÔ∏è](./2024-12-29.md)  
# 2025-01-12  
## üî± Domination  
- I wrote a script to call a locally running ollama server and generate harder comments for each file in a [side project](https://gitlab.com/bagrounds/purescript-wip/-/compare/master...20-generate-broad-documentation-with-ai).  
  - I can't run big models on my Chromebook... No GPU. But I can run llama3.2:3b.  
  - I tried to ask vscode's copilot extension to generate header comments for every file without one. It writes great comments, but would only write maybe 5-10 at a time. So it's a bit tedious.  
  - writing my own script allows me all the automation and scale I need  
    - but I'd either need an API token, which I believe requires a paid account for any of the cloud providers  
    - or I get to us the best model my Chromebook can run on 12 CPU cores and 14G of RAM  
    - With the smaller model, I've found I need to be more careful with my prompts.  
      - ask small, simple, narrow questions  
      - and still watch it sometimes ignore explicit instructions...  
    - but it's progress, and I'm having fun with it.  
      - and it's pretty awesome to leave my laptop on to write for me while I sleep  
