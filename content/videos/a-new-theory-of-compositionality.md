---
share: true
aliases:
  - ğŸ†•ğŸ§±ğŸ—£ï¸ A New Theory of Compositionality
title: ğŸ†•ğŸ§±ğŸ—£ï¸ A New Theory of Compositionality
URL: https://bagrounds.org/videos/a-new-theory-of-compositionality
Author:
Platform:
Channel: MM Azizi
tags:
youtube: https://youtu.be/G4u_pr9Ty2U
---
[Home](../index.md) > [Videos](./index.md)  
# ğŸ†•ğŸ§±ğŸ—£ï¸ A New Theory of Compositionality  
![A New Theory of Compositionality](https://youtu.be/G4u_pr9Ty2U)  
  
## ğŸ¤– AI Summary  
* ğŸ§  Thought is centered on the infinite use of finite means, generating practically infinite sentences from a limited set of words and rules \[[00:21](http://www.youtube.com/watch?v=G4u_pr9Ty2U&t=21)].  
* ğŸ’¡ Compositionality is the hallmark of human intelligence, enabling the creation of new ideas, like a cow on a beach, from familiar building blocks \[[01:08](http://www.youtube.com/watch?v=G4u_pr9Ty2U&t=68)].  
* ğŸ”‘ Cracking compositionality is necessary for truly humanlike artificial intelligence (AI), as current models are often thrown by combinations they haven't seen before \[[01:27](http://www.youtube.com/watch?v=G4u_pr9Ty2U&t=87)].  
* ğŸš§ The old definition, where meaning is the sum of its parts, is flawed, proving too loose and unbuildable for machine implementation \[[01:55](http://www.youtube.com/watch?v=G4u_pr9Ty2U&t=115)].  
* âš ï¸ Four cracks exist in the traditional foundation: what are the parts of a thought, what is the structure of a mental image, the definition allows rote memorization (idioms) to count, and it lacks a necessary spectrum \[[02:31](http://www.youtube.com/watch?v=G4u_pr9Ty2U&t=151)].  
* ğŸ†• A new theory reframes the problem using compression via Kolmogorov complexity, where an idea's complexity is the length of the shortest possible recipe to create it \[[03:12](http://www.youtube.com/watch?v=G4u_pr9Ty2U&t=192)].  
* âš–ï¸ Compositionality is a ratio: the amount of information a system can hold (expressivity) divided by the complexity of the rules (compressibility) needed to build that information \[[04:22](http://www.youtube.com/watch?v=G4u_pr9Ty2U&t=262)].  
* ğŸ“ A small, elegant instruction manual (simple rules) results in a high compositional score, while a non-compositional system (rote memorization) has an enormous instruction manual and a low score \[[05:07](http://www.youtube.com/watch?v=G4u_pr9Ty2U&t=307)].  
* âœ… Tests on synthetic data confirmed the metric's utility: scores rose with combined parts and were high when word meanings were independent, but decreased when context dependency increased \[[05:32](http://www.youtube.com/watch?v=G4u_pr9Ty2U&t=332)].  
* ğŸŒ Human languages like English, French, and Japanese scored similarly high, backing the linguistic theory that all human languages are fundamentally equally powerful and compositional \[[06:29](http://www.youtube.com/watch?v=G4u_pr9Ty2U&t=389)].  
* ğŸš€ The quantitative score provides a north star for AI design, allowing us to build models that truly generalize, handle new situations, and learn efficiently from less data \[[06:52](http://www.youtube.com/watch?v=G4u_pr9Ty2U&t=412)].  
  
## ğŸ¤” Evaluation  
* ğŸ§© The videoâ€™s core assertion, that compositionality is a key explanatory property of human intelligence permitting infinite expressive capacity from finite experience, is a traditional viewpoint acknowledged across philosophy and cognitive science, as confirmed by the Stanford Encyclopedia of Philosophy.  
* ğŸ“œ The video correctly identifies the fundamental flaw in the traditional definition: that despite strong intuition, there is no formal, measurable, and mathematical definition, a point also made in the paper A Complexity-Based Theory of Compositionality on arXiv.  
* âš™ï¸ The proposed solution, using algorithmic information theory and Kolmogorov complexity to create a quantitative metric, is supported by literature showing its relevance in measuring algorithmic efficiency, risk of overfitting, and data randomness in AI, as discussed in Kolmogorov Complexity in AI: A Guide for Investors by Alphanome.ai.  
* ğŸ†š Contrasting views exist regarding AI implementation: influential arguments held that neural networks (NNs) could not generalize compositionally, relying instead on statistical regularities, as noted in the paper [ğŸ§ ğŸ—£ï¸ğŸ’» From Frege to ChatGPT: Compositionality in Language, Cognition, and Deep Neural Networks](../articles/from-frege-to-chatgpt-compositionality-in-language-cognition-and-deep-neural-networks.md) on arXiv.  
* ğŸ§  Further contrasting perspectives challenge the human-centric compositionality claim itself: some scholars propose that human language processing is not purely compositional, but uses an asyntactic processing stream, or alternates between compositional and noncompositional strategies, explaining productivity via analogical inferences rather than formal rules, according to Constructions and Compositionality published by Cambridge University Press.  
* ğŸ’¡ Topics for further exploration include how the new metric handles non-compositional linguistic elements like metaphors and idioms in real-world data, and its direct application to improving the generalization capability of large language models.  
  
## â“ Frequently Asked Questions (FAQ)  
  
### ğŸ’¡ Q: What is the principle of compositionality and why is it crucial for creating humanlike artificial intelligence?  
ğŸ¯ A: Compositionality is the human ability to generate and understand brand new ideas by combining familiar parts, such as imagining a cow on a beach, which current artificial intelligence (AI) struggles to do, making it a key obstacle to achieving truly generalized and flexible AI.  
  
### ğŸ“ Q: How does the new compression-based theory of compositionality provide a better, measurable definition than the traditional linguistic approach?  
ğŸ”¢ A: The traditional definition is vague and allows for simple memorization to count as compositional, lacking a gradient; the new theory defines compositionality as a mathematical ratio between the system's expressive capacity and the simplicity (compressibility) of its underlying rules, providing a measurable score.  
  
### ğŸ’» Q: What is Kolmogorov complexity and how does it relate to measuring how efficient an AI system is?  
ğŸ—œï¸ A: Kolmogorov complexity, a concept from algorithmic information theory, is the length of the shortest possible recipe or program required to create an object, meaning low complexity signifies high compressibility; in AI, it is used to measure the simplicity of the rule set an intelligent system uses, where simpler rules lead to higher compositional scores and better generalization.  
  
## ğŸ“š Book Recommendations  
  
### â†”ï¸ Similar  
* [ğŸ—£ï¸ğŸ§  The Language Instinct: How the Mind Creates Language](../books/the-language-instinct-how-the-mind-creates-language.md) by Steven Pinker: Explores the evolutionary and biological basis for language, aligning with the idea of a universal, elegant system of finite means generating infinite use.  
* ğŸ§  The Computational Theory of Mind by Jerry Fodor: Argues for a classical symbolic architecture of the mind, where concepts and mental representations are inherently compositional and systematic, supporting the cognitive science foundation of the video's topic.  
  
### ğŸ†š Contrasting  
* ğŸŒ Parallel Distributed Processing: Explorations in the Microstructure of Cognition by David Rumelhart and James McClelland: The foundational text for connectionism, which models cognition through neural networks and statistical learning, offering an alternative to the symbolic, rule-based approach discussed in the video.  
* ğŸ¤” The Structure of Language: An Introduction to the Philosophy of Language by Stephen Schiffer: Examines the philosophical debates and potential counterarguments to compositionality, especially concerning context-dependence and ambiguities that challenge the simple 'sum of parts' notion.  
  
### ğŸ¨ Creatively Related  
* [â“â¡ï¸ğŸ’¡ The Book of Why: The New Science of Cause and Effect](../books/the-book-of-why.md) by Judeaa Pearl: Addresses the challenge of building AI that moves beyond statistical correlation to causal reasoning, tying into the video's goal of designing AI that can generalize and handle new, novel situations.  
* ğŸ“œ Compression: A Metaphor for Thought by David Lowenthal: Focuses on compression and information efficiency as a fundamental mechanism for human thought and creativity, paralleling the video's use of Kolmogorov complexity to define intelligence.