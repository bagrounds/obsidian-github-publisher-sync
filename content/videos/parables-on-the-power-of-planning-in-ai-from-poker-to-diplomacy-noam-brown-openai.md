---
share: true
aliases:
  - "ğŸ—ºï¸â™Ÿï¸ğŸ¤–ğŸ¤ Parables on the Power of Planning in AI: From Poker to Diplomacy: Noam Brown (OpenAI)"
title: "ğŸ—ºï¸â™Ÿï¸ğŸ¤–ğŸ¤ Parables on the Power of Planning in AI: From Poker to Diplomacy: Noam Brown (OpenAI)"
URL: https://youtu.be/eaAonE58sLU
Author: 
Platform: "#YouTube"
Channel: "[[Paul G. Allen School]]"
tags: 
---
[Home](../index.md) > [Videos](./index.md)  
# ğŸ—ºï¸â™Ÿï¸ğŸ¤–ğŸ¤ Parables on the Power of Planning in AI: From Poker to Diplomacy: Noam Brown (OpenAI)  
![Parables on the Power of Planning in AI: From Poker to Diplomacy: Noam Brown (OpenAI)](https://youtu.be/eaAonE58sLU)  
  
## ğŸ“ğŸ’ Human Notes  
- ğŸ§­ planning significantly improves machine learning system performance  
- ğŸ§  planning in machine learning systems can be implemented via e.g. ğŸ² Monte Carlo search  
- ğŸ¯ planning is useful in domains with a gap that's â›°ï¸ large between the generator and the verifier  
- ğŸ¤ the generator-verifier refers to problems where it's much easier to âœ… verify a solution than it is to âš™ï¸ generate one. ğŸ§ This type of problem presents unique challenges.  
  
## ğŸ¤– AI Summary  
**Scaling AI Through Search and Planning: Lessons from Game AI Research**  
  
### âœ¨ **Introduction**  
ğŸ§‘â€ğŸ’» Noam Brown, a leading AI researcher at OpenAI, presents insights from his work on game-playing AI, particularly in ğŸƒ **poker and strategy games like Diplomacy**. ğŸ—£ï¸ His talk explores the ğŸ§  **power of search and planning** in AI, demonstrating that ğŸ“ˆ **increasing inference compute**â€”rather than just scaling model sizeâ€”can lead to ğŸš€ **dramatic improvements in performance**.  
  
ğŸ“° This blog post distills Brownâ€™s key findings and discusses ğŸŒ **how they apply beyond games**, particularly in ğŸ¤– **large language models (LLMs) and real-world AI systems**.  
  
### ğŸ¤– **AI in Poker: From Claudico to Pluribus**  
#### â³ **Initial Challenges (2012-2015)**  
- ğŸ¤– Early poker AI was based on ğŸ’¾ **precomputed strategies**, without real-time adaptation.  
- ğŸ†š The **2015 Brains vs. AI match** saw AI *Claudico* ğŸ˜­ **lose** to human professionals, revealing flaws in its approach.  
  
#### ğŸ’¡ **Breakthrough with Search and Planning (2017-2019)**  
- ğŸ§‘â€ğŸ’» Brownâ€™s team introduced â±ï¸ **real-time search and strategic planning**, improving decision-making.  
- ğŸ† Their **2017 AI system** became the **first to defeat human poker pros**.  
- ğŸ¤– *Pluribus* (2019) achieved âœ¨ **superhuman performance in 6-player poker**, running on ğŸ’» **just 28 CPUs with a $150 training cost**.  
  
#### ğŸ”‘ **Key Lesson:**  
ğŸ§  **Strategic search massively outperforms naive scaling**â€”a ğŸ’¯ **100,000x larger model** would be needed to match the gains achieved by introducing search.  
  
**Further Reading:**  
- ğŸ“° *[Libratus: The AI that Beat Humans in Heads-Up No-Limit Poker (CMU)](https://www.cs.cmu.edu/news/2017/libratus-first-ai-beat-top-humans-poker)*  
- ğŸ“° *[Pluribus: Superhuman AI for Multiplayer Poker (Science)](https://www.science.org/doi/10.1126/science.aay2400)*  
  
### ğŸ—£ï¸ **AI in Diplomacy: Cicero and Language-Based Strategy**  
ğŸŒ Diplomacy, unlike poker, requires ğŸ—£ï¸ **natural language negotiation**. ğŸ§‘â€ğŸ’» Brownâ€™s team built ğŸ¤– **Cicero**, an AI that played at a ğŸ¥‡ **top human level**, using:  
  
1. ğŸ’¬ **Dialogue-Conditional Action Models** â€“ Predicting ğŸ¤– **not only moves but also negotiation strategies**.  
2. ğŸ” **Iterative Search** â€“ Refining plans by simulating ğŸ’­ **what other players might believe and do**.  
3. ğŸ—£ï¸ **Language Model + Planning** â€“ Instead of instantly responding like ChatGPT, Cicero ğŸ§  **strategically planned each message** (often taking â±ï¸ 10+ seconds per response).  
  
#### ğŸ”‘ **Key Lesson:**  
ğŸ¤ **AI communication and planning can be tightly integrated** to create agents that ğŸ¤” **reason, negotiate, and adapt dynamically**.  
  
**Further Reading:**  
- ğŸ“° *[Cicero: AI That Masters Diplomacy (Meta)](https://ai.meta.com/blog/cicero-ai-that-masters-diplomacy/)*  
- ğŸ“° *[Diplomacy Game Theory (CS Research Paper)](https://arxiv.org/abs/2210.01780)*  
  
### â“ **Why Planning Works: The Generator-Verifier Gap**  
ğŸ§‘â€ğŸ’» Brown introduces the ğŸ” **Generator-Verifier Gap**, where:  
- âœ… **Some problems are easier to verify than generate.**  
- â™Ÿï¸ Example: **Chess** â†’ Recognizing a winning board state is easy; ğŸ—ºï¸ **finding the path to it is hard**.  
- ğŸ¤– AI should ğŸ” **search multiple solutions, then verify which is best**â€”instead of relying on a single direct output.  
  
This principle applies to:  
- ğŸ§® **Math & Programming** â†’ Checking correctness is easier than solving from scratch.  
- âœï¸ **Proof Generation** â†’ Verifying a proof is far easier than discovering it.  
  
#### ğŸ”‘ **Key Lesson:**  
ğŸ§  Search-based AI ğŸš€ **leverages the verifier gap** to find âœ¨ **optimal solutions in complex domains**.  
  
### â¬†ï¸ **Scaling Compute: Consensus, Best-of-N, and Process Reward Models**  
ğŸ§‘â€ğŸ’» Brown discusses techniques for ğŸ“ˆ **scaling inference compute**, particularly in ğŸ¤– **LLMs and mathematical reasoning**:  
  
#### ğŸ¤ **1. Consensus Sampling**  
- ğŸ¤– AI generates â• **multiple solutions** and selects the most common.  
- ğŸ¤– Used in **Googleâ€™s Minerva**, improving math accuracy from ğŸ“‰ **33.6% to ğŸ“ˆ 50.3%**.  
  
#### ğŸ† **2. Best-of-N Selection**  
- ğŸ¤– AI generates ğŸ”¢ **N solutions** and picks the best ğŸ’° **using a reward model**.  
- âœ… Works well in structured domains like â™Ÿï¸ **chess and Sudoku**, but ğŸ˜­ struggles when ğŸ“‰ **reward models are weak**.  
  
#### âœ… **3. Process Reward Models (Step-by-Step Verification)**  
- ğŸ¤– Instead of verifying only the final answer, AI âœ… **evaluates each intermediate step**.  
- ğŸš€ **Boosted math accuracy to ğŸ“ˆ 78.2%**, surpassing all previous techniques.  
  
#### ğŸ”‘ **Key Lesson:**  
ğŸ¤– Future AI will ğŸ“ˆ **scale inference compute dynamically**, balancing â±ï¸ **speed and correctness for different tasks**.  
  
**Further Reading:**  
- ğŸ“° *[Minerva: Solving Math with LLMs (Google Research)](https://arxiv.org/abs/2206.14858)*  
- ğŸ“° *[Scaling Laws for Reward Models (DeepMind)](https://arxiv.org/abs/2302.06645)*  
  
### ğŸ”® **Future of AI: Rethinking Inference vs. Training Costs**  
ğŸ§‘â€ğŸ’» Brown argues that:  
1. ğŸ“‰ **Current AI is biased toward low-cost inference** (ChatGPT generates instant responses).  
2. âš–ï¸ **Some tasks justify high inference compute** (e.g., theorem proving, drug discovery).  
3. â“ **General methods for scaling inference are still an open research challenge**.  
  
#### ğŸ§­ **Practical Implications**  
- ğŸ¤– **LLMs should incorporate search and planning** (beyond simple token prediction).  
- ğŸ« **Academia should explore high-compute inference strategies**, as industry favors ğŸ’¸ **low-cost, high-speed AI**.  
  
**Further Reading:**  
- ğŸ“° *[The Bitter Lesson (Rich Sutton)](http://www.incompleteideas.net/IncIdeas/BitterLesson.html)*  
  
### ğŸš€ **Conclusion: The Future is Search + Learning**  
ğŸ§‘â€ğŸ’» Brown concludes with a reminder that âœ¨ **AI progress has always been driven by two scalable methods**:  
- ğŸ§  **Deep Learning (Learning from data)**  
- ğŸ§­ **Search & Planning (Efficient inference computation)**  
  
While deep learning has seen explosive growth, ğŸ§­ **search-based methods remain underutilized**. ğŸ¤– Future AI will likely â• **combine both approaches** to unlock âœ¨ **new capabilities in reasoning, problem-solving, and decision-making**.  
  
### âœ… **Final Takeaways**  
âœ… **Scaling inference compute is as important as scaling training compute**.  
âœ… **Search & planning improve AI performance across many domains**.  
âœ… **Future AI should balance speed vs. accuracy dynamically**.