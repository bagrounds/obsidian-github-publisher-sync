---
share: true
aliases:
  - "ğŸ§ ğŸ—£ï¸ğŸ’» From Frege to ChatGPT: Compositionality in Language, Cognition, and Deep Neural Networks"
title: "ğŸ§ ğŸ—£ï¸ğŸ’» From Frege to ChatGPT: Compositionality in Language, Cognition, and Deep Neural Networks"
URL: https://bagrounds.org/articles/from-frege-to-chatgpt-compositionality-in-language-cognition-and-deep-neural-networks
Author:
tags:
---
[Home](../index.md) > [Articles](./index.md)  
# [ğŸ§ ğŸ—£ï¸ğŸ’» From Frege to ChatGPT: Compositionality in Language, Cognition, and Deep Neural Networks](https://arxiv.org/pdf/2405.15164)  
  
## ğŸ¤– AI Summary  
* ğŸ§  Compositionality is the ability to compose familiar constituents into novel, complex combinations.  
* ğŸ“ The compositionality principle states a complex expression's meaning is determined by the meaning of its parts and their combination.  
* ğŸ—£ï¸ Human creative cognitive behaviors, such as language, are traditionally explained by the necessity of postulating compositionality.  
* ğŸ–¥ï¸ Classical symbolic AI systems guarantee compositionality intrinsically, but standard Deep Neural Networks (DNNs) are not architecturally designed to embody this.  
* â“ The Fodor and Pylyshyn objection traditionally targeted neural networks, claiming they lacked compositionality and were therefore non-viable cognitive models.  
* ğŸ’¡ Compositionality is required to explain **productivity** (unbounded novel thought generation) and **systematicity** (the intrinsic connection between understanding related thoughts, such as "John loves Mary" and "Mary loves John").  
* ğŸš€ Modern DNNs' impressive performance in language domains calls into question the previously hypothesized limitations of these models.  
* ğŸ“ Metalearning, or learning to learn, provides a novel perspective on how Large Language Models (LLMs) can reproduce the behavioral signatures of compositionality.  
* âœ… Recent findings suggest contemporary DNNs have plausibly surmounted the first horn of the compositionality challenge, namely the objection regarding their empirical inadequacy.  
  
## ğŸ¤” Evaluation  
* ğŸ†š The paper argues that metalearning offers LLMs a path to reproduce compositional behavior, potentially surmounting the "empirical inadequacy" critique.  
* ğŸ”¬ Conversely, other research suggests it is premature to claim modern architectures have overcome the Fodor and Pylyshyn limitations.  
* â›” This contrasting view points out that certain test cases compatible with meta-learning still provoke transduction errors and non-systematic behaviors, even within the Lake and Baroni setup (Position: Fodor and Pylyshyn's Legacy â€” Still No Human-like Systematic Compositionality in Neural Networks - OpenReview).  
* ğŸ§ Philosopher David Chalmers offers a different counter-critique to the classical Fodor/Pylyshyn argument, claiming the argument is logically defective because it would prove that the human brain, which supports compositional semantics, is also incapable of doing so (Connectionism and compositionality: Why Fodor and Pylyshyn were wrong - David Chalmers).  
* ğŸ—ºï¸ The ongoing debate highlights key topics to explore for a better understanding.  
* â“ Topics include the current status of the "mere implementation" objection, which questions whether compositional LLMs simply mimic classical symbolic systems.  
* ğŸ” Further exploration is needed to empirically distinguish true systematic generalization from statistically driven behavior that merely *resembles* compositionality.  
  
## â“ Frequently Asked Questions (FAQ)  
  
### ğŸ§© Q: What is the fundamental principle of compositionality in language and AI?  
ğŸ’¡ A: Compositionality is the principle that the meaning of a complex expression, such as a sentence, is determined solely by the meaning of its constituent parts and the structural way those parts are combined.  
  
### ğŸš« Q: What is the core challenge of compositionality that symbolic AI posed to neural networks?  
âš–ï¸ A: The compositionality challenge, famously articulated by Fodor and Pylyshyn, is a dilemma arguing that neural networks are either empirically incapable of exhibiting the systematic behavior compositionality requires, or they merely implement a classical symbolic architecture, offering no novel cognitive explanation.  
  
### ğŸ’» Q: How do modern Large Language Models (LLMs) demonstrate compositional behavior?  
ğŸ§  A: Recent research suggests that modern LLMs, particularly those leveraging metalearning or in-context learning, can successfully reproduce the behavioral signatures of compositionality, potentially overcoming the argument that these networks are empirically inadequate for modeling human cognition.  
  
## ğŸ“š Book Recommendations  
  
### â†”ï¸ Similar  
* ğŸ“˜ The Oxford Handbook of Compositionality: A comprehensive resource edited by Oxford Academic exploring compositionality across linguistics, philosophy, and computation.  
* [ğŸ¢ğŸ¤– Compositional Intelligence: Architectural Typology Through Generative AI](../books/compositional-intelligence-architectural-typology-through-generative-ai.md) by Daniel Koehler (Routledge): Investigates how compositional principles in urban design and architecture intersect with Generative AI and LLMs.  
  
### ğŸ†š Contrasting  
* ğŸŒ Parallel Distributed Processing Explorations in the Microstructure of Cognition: Volume 1 Foundations by David E Rumelhart and James L McClelland: The foundational text for connectionism, which presents an alternative, non-symbolic view of cognition that the compositionality debate fundamentally addresses.  
* â™Ÿï¸ The Algebraic Mind Integrating Connectionism and Cognitive Science by Gary F Marcus: Argues that connectionist models require innate, symbolic structure to achieve systematic generalization, upholding the necessity of a classical-style approach against pure neural networks.  
  
### ğŸ¨ Creatively Related  
* ğŸ§  The Computational Brain by Patricia S Churchland: Explores the philosophy of mind and computational neuroscience, providing a biologically grounded framework for understanding how the brain supports complex cognitive functions like language.  
* âœï¸ Rethinking Writing Instruction in the Age of AI A Universal Design for Learning Approach by Randy Laist (CAST Professional Publishing): Discusses the practical impact of LLMs on critical thinking and language education, relating the theoretical capabilities of AI to real-world applications in human communication.